{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jacobi Method Algorithm coded By John Lain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def jacobi(A, b, x0, iterations = 50, tol = 0.000001, true = None):\n",
    "    \"\"\"\n",
    "    Solves a system of linear equations using the Jacobi method\n",
    "    Expected inputs:\n",
    "    A: nxn matrix\n",
    "    b: column vector of length n\n",
    "    x0: initial guess for solution\n",
    "    tol: error between true and numerical solution\n",
    "    true: true solution\n",
    "    \"\"\"\n",
    "    count = 0\n",
    "    x = x0\n",
    "    d = np.diagonal(A)\n",
    "    LU = A - np.diag(d)\n",
    "    n = np.size(b)\n",
    "    if type(true) == np.ndarray:\n",
    "        while max(abs(true - x)) > tol:\n",
    "            count = count + 1\n",
    "            x = (1/d)*(b - LU@x)\n",
    "        print(f\"Number of iterations = {count}\")\n",
    "        return(x)\n",
    "    else:\n",
    "        for i in range(iterations):\n",
    "            x = (1/d)*(b - LU@x)\n",
    "        return(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## When writing this code, we run into a few errors: \n",
    "Creating different scenarios for the method alorithm when we include the true solution in our input lead to some issues, and had to use the code \"type(true) == np.ndarray\" which was not clear at first. \n",
    "\n",
    "At first, John tried to use matrix multiplication when multiplying by d inverse (like is done in the mathmatical algoritm), however, d is stored as an array in our code and because of vectorized nature of numpy, the solution was to just use the normal multiplication operator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the Algorithm\n",
    "\n",
    "- Example from HW 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of iterations = 35\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.99999991, 0.99999982, 0.99999974, 0.99999966, 0.99999959,\n",
       "       0.99999953, 0.99999948, 0.99999943, 0.9999994 , 0.99999937,\n",
       "       0.99999936, 0.99999934, 0.99999933, 0.99999932, 0.99999932,\n",
       "       0.99999932, 0.99999932, 0.99999931, 0.99999931, 0.99999931,\n",
       "       0.99999931, 0.99999931, 0.99999931, 0.99999931, 0.99999931,\n",
       "       0.99999931, 0.99999931, 0.99999931, 0.99999931, 0.99999931,\n",
       "       0.99999931, 0.99999931, 0.99999931, 0.99999931, 0.99999931,\n",
       "       0.99999931, 0.99999931, 0.99999931, 0.99999931, 0.99999931,\n",
       "       0.99999931, 0.99999931, 0.99999931, 0.99999931, 0.99999931,\n",
       "       0.99999931, 0.99999931, 0.99999931, 0.99999931, 0.99999931,\n",
       "       0.99999931, 0.99999931, 0.99999931, 0.99999931, 0.99999931,\n",
       "       0.99999931, 0.99999931, 0.99999931, 0.99999931, 0.99999931,\n",
       "       0.99999931, 0.99999931, 0.99999931, 0.99999931, 0.99999931,\n",
       "       0.99999931, 0.99999931, 0.99999931, 0.99999931, 0.99999931,\n",
       "       0.99999931, 0.99999931, 0.99999931, 0.99999931, 0.99999931,\n",
       "       0.99999931, 0.99999931, 0.99999931, 0.99999931, 0.99999931,\n",
       "       0.99999931, 0.99999931, 0.99999931, 0.99999932, 0.99999932,\n",
       "       0.99999932, 0.99999932, 0.99999933, 0.99999934, 0.99999936,\n",
       "       0.99999937, 0.9999994 , 0.99999943, 0.99999948, 0.99999953,\n",
       "       0.99999959, 0.99999966, 0.99999974, 0.99999982, 0.99999991])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L = np.diag(-np.ones(99), -1)\n",
    "U = np.diag(-np.ones(99), 1)\n",
    "d = np.diag(3*np.ones(100))\n",
    "A = L + U + d\n",
    "b = np.array([2] + [1]*98 + [2])\n",
    "x = jacobi(A, b, np.zeros(100), tol = 0.000001, true = np.ones(100))\n",
    "x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm written by Chat GPT 3.5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jacobi_GPT(A, b, x0, tol=1e-6, max_iter=1000):\n",
    "    n = len(b)\n",
    "    x = np.array(x0, dtype=float)\n",
    "    x_new = np.zeros_like(x)\n",
    "    \n",
    "    for _ in range(max_iter):\n",
    "        for i in range(n):\n",
    "            sum_ = np.dot(A[i, :i], x[:i]) + np.dot(A[i, i+1:], x[i+1:])\n",
    "            x_new[i] = (b[i] - sum_) / A[i, i]\n",
    "        \n",
    "        if np.linalg.norm(x_new - x) < tol:\n",
    "            return x_new\n",
    "        \n",
    "        x = np.copy(x_new)\n",
    "    \n",
    "    raise ValueError(\"Jacobi method did not converge within the maximum number of iterations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Chat gpts code stops the alorithm when the difference between the new value and the last value is less than a certain amount, which is different than our code which uses a set number of iterations (or requires the solution which is helpful for testing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Chat GPT's code with the same example from HW 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.99999997 0.99999995 0.99999992 0.9999999  0.99999988 0.99999987\n",
      " 0.99999985 0.99999984 0.99999983 0.99999982 0.99999981 0.99999981\n",
      " 0.9999998  0.9999998  0.9999998  0.9999998  0.9999998  0.9999998\n",
      " 0.9999998  0.9999998  0.9999998  0.9999998  0.9999998  0.9999998\n",
      " 0.9999998  0.9999998  0.9999998  0.9999998  0.9999998  0.9999998\n",
      " 0.9999998  0.9999998  0.9999998  0.9999998  0.9999998  0.9999998\n",
      " 0.9999998  0.9999998  0.9999998  0.9999998  0.9999998  0.9999998\n",
      " 0.9999998  0.9999998  0.9999998  0.9999998  0.9999998  0.9999998\n",
      " 0.9999998  0.9999998  0.9999998  0.9999998  0.9999998  0.9999998\n",
      " 0.9999998  0.9999998  0.9999998  0.9999998  0.9999998  0.9999998\n",
      " 0.9999998  0.9999998  0.9999998  0.9999998  0.9999998  0.9999998\n",
      " 0.9999998  0.9999998  0.9999998  0.9999998  0.9999998  0.9999998\n",
      " 0.9999998  0.9999998  0.9999998  0.9999998  0.9999998  0.9999998\n",
      " 0.9999998  0.9999998  0.9999998  0.9999998  0.9999998  0.9999998\n",
      " 0.9999998  0.9999998  0.9999998  0.9999998  0.99999981 0.99999981\n",
      " 0.99999982 0.99999983 0.99999984 0.99999985 0.99999987 0.99999988\n",
      " 0.9999999  0.99999992 0.99999995 0.99999997]\n",
      "4.832771325347096e-07\n"
     ]
    }
   ],
   "source": [
    "L = np.diag(-np.ones(99), -1)\n",
    "U = np.diag(-np.ones(99), 1)\n",
    "d = np.diag(3*np.ones(100))\n",
    "A = L + U + d\n",
    "b = np.array([2] + [1]*98 + [2])\n",
    "x_GPT = jacobi_GPT(A, b, np.zeros(100), tol = 0.000001)\n",
    "print(x_GPT)\n",
    "print(max(abs(x_GPT - x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The GPT 3.5 code produced a result extremely close to the result of our code. Asking Chat GPT to write the code took about 2 minutes, whereas the code we wrote took about an hour with trial and error being required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.99999997 1.99999994 0.99999997]\n",
      "[0.99999905 1.99999905 0.99999905]\n"
     ]
    }
   ],
   "source": [
    "A = np.array([[2, -1, 0], [-1, 2, -1], [0, -1, 2]])\n",
    "b = np.array([0, 2, 0])\n",
    "x = jacobi(A, b, np.zeros(3))\n",
    "print(x)\n",
    "x_gpt = jacobi_GPT(A, b, np.zeros(3))\n",
    "print(x_gpt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above is another example from HW 3\n",
    "\n",
    "- Both of the functions produced results close to the true solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent Method By Zhenyuan Ni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum at: [4.01734511e-06 4.01734511e-06]\n",
      "Minimum at: [ 1.23023192e-97  4.93986584e+41 -6.29828712e+29  4.07127679e+30\n",
      " -2.35055275e+30]\n",
      "1.4167495700510191e+121\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def gradient_descent(gradient, start, learn_rate, n_iterations, tolerance):\n",
    "    vector = start\n",
    "    for _ in range(n_iterations):\n",
    "        diff = -learn_rate * gradient(vector)\n",
    "        if np.all(np.abs(diff) <= tolerance):\n",
    "            break\n",
    "        vector += diff\n",
    "    return vector\n",
    "\n",
    "\n",
    "# Example usage\n",
    "# Define the gradient of your function here\n",
    "def gradient_function(v):\n",
    "    # Example for a 2D function: f(x, y) = x^2 + y^2\n",
    "    return np.array([2 * v[0], 2 * v[1]])\n",
    "\n",
    "\n",
    "# Starting point (can be any point in your domain)\n",
    "start_point = np.array([10.0, 10.0])\n",
    "\n",
    "# Learning rate\n",
    "learn_rate = 0.1\n",
    "\n",
    "# Number of iterations\n",
    "n_iterations = 1000\n",
    "\n",
    "# Tolerance for stopping criterion\n",
    "tolerance = 1e-6\n",
    "\n",
    "minimum = gradient_descent(gradient_function, start_point, learn_rate, n_iterations, tolerance)\n",
    "print(\"Minimum at:\", minimum)\n",
    "\n",
    "\n",
    "# Multivariable case\n",
    "def func(v):\n",
    "    return v[0]**2 - v[1] * v[2]**2 + v[2] * v[3] * v[4]**2\n",
    "\n",
    "\n",
    "def gradient_function1(v):\n",
    "    # Example for a 2D function: f(x, y) = x1^2 - x2 * x3^2 + x3 * x4 * x5^2\n",
    "    return np.array([2 * v[0], -v[1], 2*v[2] + v[3] + v[4], v[2] + v[4], v[3] + v[4]])\n",
    "\n",
    "\n",
    "start_point = np.array([1.0, 2.0, 3.0, 4.0, 5.0])\n",
    "minimum = gradient_descent(gradient_function1, start_point, learn_rate, n_iterations, tolerance)\n",
    "print(\"Minimum at:\", minimum)\n",
    "ylist = []\n",
    "for x1 in range(10):\n",
    "    for x2 in range(10):\n",
    "        for x3 in range(10):\n",
    "            for x4 in range(10):\n",
    "                for x5 in range(10):\n",
    "                    ylist.append(func([1 + 0.1 * x1, 4 + 0.1 * x2, -6 + 0.1 * x3, 4 + 0.1 * x4, -2 + 0.1 * x5]))\n",
    "miny = np.min(ylist) - func(minimum)\n",
    "print(miny)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chatgpt 3.5 Gradient Descent method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chat Gpt notes that a common optimization for better efficiency is to use vectorized coding instead:\n",
    "\n",
    "- One common optimization is to use vectorized operations whenever possible to leverage the computational efficiency of NumPy. \n",
    "- Additionally, we can adjust the step size dynamically during optimization to improve convergence. \n",
    "\n",
    "Here's the optimized version of the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum at: [1.20459505 1.20459505]\n",
      "Minimum at: [0.12045854        nan        nan        nan        nan]\n",
      "nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/xh/4xpwy9dj7glg3lpzyn0wwqgw0000gn/T/ipykernel_4675/1267282877.py:43: RuntimeWarning: overflow encountered in scalar power\n",
      "  grad[1] = -v[2]**2\n",
      "/var/folders/xh/4xpwy9dj7glg3lpzyn0wwqgw0000gn/T/ipykernel_4675/1267282877.py:44: RuntimeWarning: overflow encountered in scalar power\n",
      "  grad[2] = -2 * v[1] * v[2] + v[3] * v[4]**2\n",
      "/var/folders/xh/4xpwy9dj7glg3lpzyn0wwqgw0000gn/T/ipykernel_4675/1267282877.py:45: RuntimeWarning: overflow encountered in scalar power\n",
      "  grad[3] = v[2] * v[4]**2\n",
      "/var/folders/xh/4xpwy9dj7glg3lpzyn0wwqgw0000gn/T/ipykernel_4675/1267282877.py:46: RuntimeWarning: overflow encountered in scalar multiply\n",
      "  grad[4] = 2 * v[2] * v[3] * v[4]\n",
      "/var/folders/xh/4xpwy9dj7glg3lpzyn0wwqgw0000gn/T/ipykernel_4675/1267282877.py:44: RuntimeWarning: invalid value encountered in scalar add\n",
      "  grad[2] = -2 * v[1] * v[2] + v[3] * v[4]**2\n",
      "/var/folders/xh/4xpwy9dj7glg3lpzyn0wwqgw0000gn/T/ipykernel_4675/1267282877.py:9: RuntimeWarning: invalid value encountered in add\n",
      "  vector += diff\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def gradient_descent(gradient, start, learn_rate, n_iterations, tolerance):\n",
    "    vector = start\n",
    "    for _ in range(n_iterations):\n",
    "        diff = -learn_rate * gradient(vector)\n",
    "        if np.all(np.abs(diff) <= tolerance):\n",
    "            break\n",
    "        vector += diff\n",
    "        # Adjust learning rate dynamically\n",
    "        learn_rate *= 0.9  # or any other suitable decay factor\n",
    "    return vector\n",
    "\n",
    "# Example usage\n",
    "# Define the gradient of your function here\n",
    "def gradient_function(v):\n",
    "    # Example for a 2D function: f(x, y) = x^2 + y^2\n",
    "    return np.array([2 * v[0], 2 * v[1]])\n",
    "\n",
    "# Starting point (can be any point in your domain)\n",
    "start_point = np.array([10.0, 10.0])\n",
    "\n",
    "# Learning rate\n",
    "learn_rate = 0.1\n",
    "\n",
    "# Number of iterations\n",
    "n_iterations = 1000\n",
    "\n",
    "# Tolerance for stopping criterion\n",
    "tolerance = 1e-6\n",
    "\n",
    "minimum = gradient_descent(gradient_function, start_point, learn_rate, n_iterations, tolerance)\n",
    "print(\"Minimum at:\", minimum)\n",
    "\n",
    "# Multivariable case\n",
    "def func(v):\n",
    "    return v[0]**2 - v[1] * v[2]**2 + v[2] * v[3] * v[4]**2\n",
    "\n",
    "def gradient_function1(v):\n",
    "    # Gradient of the function f(v) = v[0]^2 - v[1] * v[2]^2 + v[2] * v[3] * v[4]^2\n",
    "    grad = np.zeros_like(v)\n",
    "    grad[0] = 2 * v[0]\n",
    "    grad[1] = -v[2]**2\n",
    "    grad[2] = -2 * v[1] * v[2] + v[3] * v[4]**2\n",
    "    grad[3] = v[2] * v[4]**2\n",
    "    grad[4] = 2 * v[2] * v[3] * v[4]\n",
    "    return grad\n",
    "\n",
    "\n",
    "start_point = np.array([1.0, 2.0, 3.0, 4.0, 5.0])\n",
    "minimum = gradient_descent(gradient_function1, start_point, learn_rate, n_iterations, tolerance)\n",
    "print(\"Minimum at:\", minimum)\n",
    "\n",
    "# No need to search for minimum manually\n",
    "# Minimum value can be calculated directly from the function\n",
    "miny = func(minimum)\n",
    "print(miny)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that there was an overflow issue with the results in the code. This is primarily due to:\n",
    "\n",
    "- the gradient vector was properly updated, and for multivariate cases we see this is an issue that can arise for wrong initial guesses, \n",
    "- The need to change hyperparameters (learning rate, iterations etc.)\n",
    "\n",
    "This can be explored more so in another course but this is very interesting to see what can cause runtime warnings "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
